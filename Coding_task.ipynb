{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coding_task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW2G64Q8ypsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import pathlib\n",
        "import imageio\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from itertools import islice\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, roc_auc_score"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaXkAOtTVtPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_paths = []\n",
        "for root, directories, filenames in os.walk('development'):\n",
        "  for filename in filenames: \n",
        "    training_paths.append(os.path.join(root,filename))  \n",
        "\n",
        "# split the dataset in train and validation set (holdout stratified)\n",
        "# lab1 = np.ones((380,), dtype=int)\n",
        "# lab2 = np.zeros((380, ), dtype=int)\n",
        "# lab = np.concatenate((lab1, lab2))\n",
        "# skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "# split = skf.split(training_paths, lab)\n",
        "# select = 0\n",
        "# train_ix, val_ix = next(islice(split, select, select + 1))\n",
        "# training_paths = np.array(training_paths)\n",
        "# train_ids = training_paths[train_ix.astype(int)]\n",
        "# valid_ids = training_paths[val_ix]\n",
        "\n",
        "# split the dataset in train and validation set (use some folder for validation)\n",
        "training_paths = np.array(training_paths)\n",
        "train_ids1 = training_paths[0:250]\n",
        "train_ids2 = training_paths[380: 630]\n",
        "valid_ids1 = training_paths[250: 380]\n",
        "valid_ids2 = training_paths[630: 760]\n",
        "train_ids = np.concatenate((train_ids1, train_ids2))\n",
        "valid_ids = np.concatenate((valid_ids1, valid_ids2))\n",
        "\n",
        "class DataGenerator(Dataset):\n",
        "\n",
        "  def __init__(self, images_path, augmentations=False):\n",
        "    self.images_path = images_path\n",
        "    self.augmentations = augmentations\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    im_path = self.images_path[index]\n",
        "    input_image = Image.open(im_path)\n",
        "    preprocess = None\n",
        "    if self.augmentations:\n",
        "      preprocess = transforms.Compose([\n",
        "        transforms.Resize(400),\n",
        "        transforms.CenterCrop(400),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomGrayscale(p=0.1),\n",
        "        transforms.ColorJitter(brightness = 0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "      ])\n",
        "    else:\n",
        "      preprocess = transforms.Compose([\n",
        "        transforms.Resize(400),\n",
        "        transforms.CenterCrop(400),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "      ])\n",
        "    input_tensor = preprocess(input_image)\n",
        "    path = im_path.split(\"/\")\n",
        "    label = 0 if path[1] == \"real\" else 1\n",
        "    return input_tensor, label, im_path\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images_path)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-S2EPvTZ8MD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = DataGenerator(train_ids, True)\n",
        "train_data_loader = DataLoader(train_data,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "valid_data = DataGenerator(valid_ids, False)\n",
        "valid_data_loader = DataLoader(valid_data,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "def validation_score(model, valid_data, device):\n",
        "  tot_loss = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for xb, yb, _ in valid_data:\n",
        "      \n",
        "      model.to(device)\n",
        "      xb = xb.to(device)\n",
        "      yb = yb.to(device)\n",
        "\n",
        "      pred = model(xb)\n",
        "      pred_label = torch.argmax(pred, dim=1)\n",
        "      \n",
        "      loss = loss_func(pred, yb)\n",
        "      tot_loss += loss\n",
        "      # alternative loss\n",
        "      # for p, y in zip(pred_label, yb):\n",
        "      #  if p!=y:\n",
        "      #    tot_loss += 1\n",
        "\n",
        "  return tot_loss/len(valid_ids)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02Rx3kxvMiQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model: resnet50 with a different classifier\n",
        "model = torchvision.models.resnet50(pretrained=True)\n",
        "model.fc = nn.Linear(2048, 2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AOIZNfmOLGL",
        "colab_type": "text"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDzfW9MVyyWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "45e6c808-cf30-4aca-d6df-f14bdbe8c33b"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "# settings\n",
        "n_epochs = 8\n",
        "lr = 0.0001\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "opt = optim.Adam(params, lr=lr, weight_decay=0.1)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=4, gamma=0.1)\n",
        "vd_score = None\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  tot_loss = 0\n",
        "  model.train()\n",
        "  \n",
        "  for xb, yb, _ in train_data_loader:\n",
        "    model.to(device)\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "\n",
        "    tot_loss += loss\n",
        "    if opt is not None:\n",
        "          loss.backward()\n",
        "          opt.step()\n",
        "          opt.zero_grad()\n",
        "  \n",
        "  if lr_scheduler is not None:\n",
        "    lr_scheduler.step()\n",
        "  print(f'Loss: {tot_loss / len(train_ids)}')\n",
        "  val = validation_score(model, valid_data_loader, device)\n",
        "  print(f'Val loss: {val}')\n",
        "\n",
        "  # save best weights\n",
        "  if vd_score is None or val < vd_score:\n",
        "    vd_score = val\n",
        "    torch.save(model.state_dict(), 'weights/uam_weights_softmax_best.pth')\n",
        "\n",
        "  # save weights every two epochs\n",
        "  if epoch % 2 == 1:\n",
        "    torch.save(model.state_dict(), f'weights/uam_weights_softmax{epoch}.pth')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Loss: 0.021726055070757866\n",
            "Val loss: 0.0147639075294137\n",
            "Loss: 0.0034787566401064396\n",
            "Val loss: 0.006595338229089975\n",
            "Loss: 0.0024388059973716736\n",
            "Val loss: 0.009996622800827026\n",
            "Loss: 0.0035369780380278826\n",
            "Val loss: 0.010646982118487358\n",
            "Loss: 0.005149087402969599\n",
            "Val loss: 0.004087382927536964\n",
            "Loss: 0.0012290800223127007\n",
            "Val loss: 0.003794043557718396\n",
            "Loss: 0.0012018807465210557\n",
            "Val loss: 0.002362791681662202\n",
            "Loss: 0.002455664100125432\n",
            "Val loss: 0.0023576002568006516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rJ7pujYSoxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save best weights on Google Drive\n",
        "!cp -r weights/uam_weights_softmax_best.pth drive/My\\ Drive/"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcdiv1glXnUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "22b8aa6f-fe77-4049-e82e-e24252d773d3"
      },
      "source": [
        "# Use best weights for evaluation\n",
        "model.load_state_dict(torch.load(\"weights/uam_weights_softmax_best.pth\"))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H6JIX25OQd1",
        "colab_type": "text"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRg5vsw6PZvz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "82649ec9-70d9-4534-d3f4-27bf69572a8d"
      },
      "source": [
        "evaluation_paths = []\n",
        "for root, directories, filenames in os.walk('evaluation'):\n",
        "  for filename in filenames: \n",
        "    evaluation_paths.append(os.path.join(root,filename))\n",
        "\n",
        "evaluation_data = DataGenerator(evaluation_paths, False)\n",
        "evaluation_data_loader = DataLoader(evaluation_data,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "tot_loss = 0\n",
        "err = 0\n",
        "model.eval()\n",
        "\n",
        "testy = []\n",
        "prob_fake = []\n",
        "results = []\n",
        "false_positive = 0\n",
        "false_negative = 0\n",
        "true_positive = 0\n",
        "true_negative = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for xb, yb, pathb in evaluation_data_loader:\n",
        "    model.to(device)\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "    tot_loss += loss\n",
        "    pred_label = torch.argmax(pred, dim=1)\n",
        "\n",
        "    yb = yb.cpu().numpy()\n",
        "    for p, y, p2, x, pb in zip(pred_label, yb, pred, xb, pathb):\n",
        "      score = F.softmax(p2)\n",
        "      result = {\n",
        "          'image_path': pb,\n",
        "          'true_label': y,\n",
        "          'predicted_label': p.item(),\n",
        "          'score_real': score[0].item(),\n",
        "          'score_fake': score[1].item()\n",
        "      }\n",
        "      results.append(result)\n",
        "      \n",
        "      # wrong prediction\n",
        "      if p!=y:\n",
        "        err = err + 1\n",
        "        print(f'Wrong prediction - img path: {pb}, Prediction: {score}')\n",
        "    \n",
        "        if p:\n",
        "          false_positive = false_positive + 1\n",
        "        else:\n",
        "          false_negative = false_negative + 1\n",
        "      \n",
        "      else:\n",
        "        if p:\n",
        "          true_positive = true_positive + 1\n",
        "        else:\n",
        "          true_negative = true_negative + 1\n",
        "\n",
        "      # data for ROC\n",
        "      testy.append(y)\n",
        "      prob_fake.append(score[1].cpu())\n",
        "\n",
        "  auc = roc_auc_score(testy, prob_fake)\n",
        "  print(f'AUC: {auc}')\n",
        "  fpr, tpr, _ = roc_curve(testy, prob_fake, drop_intermediate=False)\n",
        "  plt.plot(fpr, tpr, marker='.')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.show()\n",
        "  print(f'Wrong predictions: {err}')\n",
        "  print(f'Average loss: {tot_loss/len(evaluation_paths)}')\n",
        "  print(f'Precision: {true_positive / (true_positive + false_positive)}')\n",
        "  print(f'Recall: {true_positive / (true_positive + false_negative)}')\n",
        "\n",
        "# write results\n",
        "test_df = pd.DataFrame(results, columns=['image_path', 'true_label', 'predicted_label', \n",
        "                                         'score_real', 'score_fake'])\n",
        "test_df.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wrong prediction - img path: evaluation/fake/0045_fake/fake.011473.jpg, Prediction: tensor([0.5721, 0.4279], device='cuda:0')\n",
            "Wrong prediction - img path: evaluation/fake/0045_fake/fake.011173.jpg, Prediction: tensor([0.6791, 0.3209], device='cuda:0')\n",
            "Wrong prediction - img path: evaluation/fake/0045_fake/fake.011172.jpg, Prediction: tensor([0.5762, 0.4238], device='cuda:0')\n",
            "Wrong prediction - img path: evaluation/fake/0045_fake/fake.011499.jpg, Prediction: tensor([0.6246, 0.3754], device='cuda:0')\n",
            "Wrong prediction - img path: evaluation/fake/0041_fake/fake.009315.jpg, Prediction: tensor([0.8107, 0.1893], device='cuda:0')\n",
            "Wrong prediction - img path: evaluation/fake/0044_fake/fake.010578.jpg, Prediction: tensor([0.6537, 0.3463], device='cuda:0')\n",
            "AUC: 0.9963636363636363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Wrong predictions: 6\n",
            "Average loss: 0.01519872434437275\n",
            "Precision: 1.0\n",
            "Recall: 0.94\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}